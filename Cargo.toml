[package]
name = "llm-scratch"
version = "0.1.0"
edition = "2024"
authors = ["tangxiangong <tangxiangong@gmail.com>"]
description = "Build a Large Language Model (From Scratch) -- Rust Edition"
publish = false
license = "Apache-2.0 OR MIT"

[features]
default = []
mkl = ["candle-core/mkl", "candle-nn/mkl", "intel-mkl-src"]
accelerate = ["candle-core/accelerate", "candle-nn/accelerate", "accelerate-src"]
cuda = ["candle-core/cudnn", "candle-nn/cudnn"]

[dependencies]
anyhow = "1.0.99"
tokenizers = { version = "0.22.0", features = ["http", "rustls-tls"] }

[target.'cfg(all(target_arch = "aarch64", target_os = "macos"))'.dependencies]
candle-core = { version = "0.9.1", features = ["metal"] }
candle-nn = { version = "0.9.1", features = ["metal"] }
[target.'cfg(not(all(target_arch = "aarch64", target_os = "macos")))'.dependencies]
accelerate-src = { version = "0.3.2", optional = true }
candle-core = { version = "0.9.1" }
candle-nn = { version = "0.9.1" }
intel-mkl-src = { version = "0.8.1", optional = true }
